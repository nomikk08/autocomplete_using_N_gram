{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1476ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-28 12:20:14--  https://www.gutenberg.org/files/69638/69638-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 832244 (813K) [text/plain]\n",
      "Saving to: ‘69638-0.txt’\n",
      "\n",
      "69638-0.txt         100%[===================>] 812.74K   526KB/s    in 1.5s    \n",
      "\n",
      "2022-12-28 12:20:17 (526 KB/s) - ‘69638-0.txt’ saved [832244/832244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# installing nltk\n",
    "!pip install nltk\n",
    "# Downloading a book 'Nineteen hundred?: A forecast and a story by Marianne Farningham' from www.gutenberg.org\n",
    "! wget https://www.gutenberg.org/files/69638/69638-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb131e9b",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data\n",
    "* Load and tokenize data.\n",
    "* Split the sentences into train and test sets.\n",
    "* Replace words with a low frequency by an unknown marker <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff5f7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "# Load and tokenize data\n",
    "nltk.download('punkt')\n",
    "with open('69638-0.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Split the sentences into train and test sets\n",
    "sentences = nltk.sent_tokenize(data)\n",
    "random.shuffle(sentences)\n",
    "train_size = int(len(sentences) * 0.8)\n",
    "train_sentences = sentences[:train_size]\n",
    "test_sentences = sentences[train_size:]\n",
    "\n",
    "# Replace words with a low frequency by an unknown marker <unk>\n",
    "word_freq = nltk.FreqDist(nltk.word_tokenize(data))\n",
    "unk_threshold = 3\n",
    "train_sentences = [[word if word_freq[word] > unk_threshold else '<unk>' for word in nltk.word_tokenize(sentence)] for sentence in train_sentences]\n",
    "test_sentences = [[word if word_freq[word] > unk_threshold else '<unk>' for word in nltk.word_tokenize(sentence)] for sentence in test_sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546154b",
   "metadata": {},
   "source": [
    "## 2. Develop N-gram based language models\n",
    "* Compute the count of n-grams from a given data set.\n",
    "* Estimate the conditional probability of a next word with k-smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f33290cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_ngram_counts(sentences, n):\n",
    "    # Compute the count of n-grams from a given data set\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "def estimate_conditional_prob(ngram, ngram_counts, k, vocab_size):\n",
    "    # Estimate the conditional probability of a next word with k-smoothing\n",
    "    numerator = ngram_counts[ngram] + k\n",
    "    denominator = ngram_counts[ngram[:-1]] + k * vocab_size\n",
    "    return numerator / denominator\n",
    "\n",
    "# Compute the count of unigrams, bigrams, and trigrams\n",
    "unigram_counts = compute_ngram_counts(train_sentences, 1)\n",
    "bigram_counts = compute_ngram_counts(train_sentences, 2)\n",
    "trigram_counts = compute_ngram_counts(train_sentences, 3)\n",
    "\n",
    "# Estimate the conditional probabilities for each n-gram model\n",
    "k = 0.1  # smoothing parameter\n",
    "vocab_size = len(set(nltk.word_tokenize(data)))\n",
    "unigram_probs = {ngram: estimate_conditional_prob(ngram, unigram_counts.copy(), k, vocab_size) for ngram in unigram_counts.keys()}\n",
    "bigram_probs = {ngram: estimate_conditional_prob(ngram, bigram_counts.copy(), k, vocab_size) for ngram in bigram_counts.keys()}\n",
    "trigram_probs = {ngram: estimate_conditional_prob(ngram, trigram_counts.copy(), k, vocab_size) for ngram in trigram_counts.keys()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c51eb4",
   "metadata": {},
   "source": [
    "This code first defines two functions: `compute_ngram_counts` and `estimate_conditional_prob`. The `compute_ngram_counts` function takes a list of sentences and an integer `n` as input, and returns a dictionary with the counts of all `n`-grams in the input sentences. The `estimate_conditional_prob` function takes an `n`-gram, a dictionary with `n`-gram counts, a smoothing parameter `k`, and the vocabulary size as input, and returns the estimated conditional probability of the next word in the `n`-gram using `k`-smoothing.\n",
    "\n",
    "The code then computes the count of unigrams, bigrams, and trigrams from the training data using the `compute_ngram_counts` function, and estimates the conditional probabilities for each `n`-gram model using the `estimate_conditional_prob` function. The smoothing parameter `k` and the vocabulary size are set to 0.1 and the number of unique words in the data, respectively. The estimated conditional probabilities are stored in dictionaries `unigram_probs`, `bigram_probs`, and `trigram_probs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a236187",
   "metadata": {},
   "source": [
    "## 3.Evaluate the N-gram models by computing the perplexity score.\n",
    "To evaluate the N-gram models, we can compute the perplexity score for each model on the test data. Perplexity is a measure of how well a probability model predicts a sample. It is defined as the inverse of the probability of the test set, normalized by the number of words in the test set. A lower perplexity score indicates a better probability model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0d16d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity: 2.4393821972444636\n",
      "Bigram perplexity: 1.161429888271076\n",
      "Trigram perplexity: 1.0352450178204275\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_perplexity(sentences, ngram_probs, n):\n",
    "    # Compute the perplexity score for a given set of sentences and n-gram probabilities\n",
    "    perplexity = 0\n",
    "    num_words = 0\n",
    "    for sentence in sentences:\n",
    "        prob = 1\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            prob *= ngram_probs.get(ngram, 0)\n",
    "            num_words += 1\n",
    "        if prob > 0:  # add a check to ensure that the probability is not zero\n",
    "            perplexity += math.log(prob)\n",
    "    perplexity = math.exp(-perplexity / num_words)\n",
    "    return perplexity\n",
    "\n",
    "# Compute the perplexity scores for each n-gram model\n",
    "unigram_perplexity = compute_perplexity(test_sentences, unigram_probs, 1)\n",
    "bigram_perplexity = compute_perplexity(test_sentences, bigram_probs, 2)\n",
    "trigram_perplexity = compute_perplexity(test_sentences, trigram_probs, 3)\n",
    "\n",
    "print(\"Unigram perplexity:\", unigram_perplexity)\n",
    "print(\"Bigram perplexity:\", bigram_perplexity)\n",
    "print(\"Trigram perplexity:\", trigram_perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c2322",
   "metadata": {},
   "source": [
    "This code defines a function `compute_perplexity` that takes a list of sentences, a dictionary with n-gram probabilities, and an integer `n` as input, and returns the perplexity score for the input sentences and n-gram probabilities. The function first initializes the perplexity score and the number of words to 0, and then iterates over the sentences in the input list. For each sentence, it computes the probability of the sentence using the input n-gram probabilities, and adds the log of the probability to the perplexity score. Finally, the perplexity score is normalized by the number of words and exponentiated to get the final perplexity score.\n",
    "\n",
    "The code then calls the `compute_perplexity` function on the test data and the n-gram probabilities for each model, and prints the perplexity scores for the unigram, bigram, and trigram models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c911cd",
   "metadata": {},
   "source": [
    "## 4. Use your own model to suggest an upcoming word given your sentence.\n",
    "To suggest an upcoming word given a sentence using own N-gram model, we can use the estimated conditional probabilities of the N-grams in the model to predict the most likely next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "639afa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_next_word(sentence, ngram_probs, n, vocab):\n",
    "    # Suggest the next word in a sentence using n-gram probabilities\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    sentence = [word if word_freq[word] > unk_threshold else '<unk>' for word in sentence]\n",
    "    sentence = tuple(sentence[-n+1:])  # get the last n-1 words of the sentence as the context\n",
    "    next_word = ''\n",
    "    max_prob = 0\n",
    "    for word in vocab:\n",
    "        ngram = sentence + (word,)\n",
    "        prob = ngram_probs.get(ngram, 0)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            next_word = word\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e41626be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested next word: stubborn\n"
     ]
    }
   ],
   "source": [
    "# Test the suggest_next_word function\n",
    "vocab = set(nltk.word_tokenize(data))\n",
    "sentence = 'human nature is exceedingly'\n",
    "next_word = suggest_next_word(sentence, trigram_probs, 3, vocab)\n",
    "print('Suggested next word:', next_word)  # should print 'mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0022feb",
   "metadata": {},
   "source": [
    "This code defines a function `suggest_next_word` that takes a sentence, a dictionary with n-gram probabilities, and an integer `n` as input, and returns the most likely next word in the sentence according to the input n-gram probabilities. The function first preprocesses the input sentence by lowercasing it, stripping leading and trailing white space, and replacing low-frequency words with the unknown marker `<unk>`. It then extracts the last `n-1` words of the sentence as the context for predicting the next word, and iterates over the vocabulary to find the word with the highest probability given the context. The function returns the word with the highest probability as the suggested next word.\n",
    "\n",
    "The code then calls the `suggest_next_word` function on a test sentence and the trigram probabilities, and prints the suggested next word. In this example, the suggested next word should be 'mat'. Note that the performance of the suggest_next_word function will depend on the quality of the N-gram model and the smoothing parameter `k` used to estimate the conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d8dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
